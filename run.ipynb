{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7a60a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:05:13.666902Z",
     "start_time": "2021-05-31T07:05:12.293507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.2.4\n",
      "numpy version: 1.20.2\n",
      "seaborn version: 0.11.1\n",
      "matplotlib version: 3.4.1\n",
      "sklearn version: 0.24.2\n",
      "transformers version: 4.5.1\n",
      "torch version: 1.8.1+cu102\n",
      "training device: (device(type='cuda', index=0), 'TITAN Xp')\n",
      "signature: jh_ELECTRA_5M_31D_16H_5M\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(\"ggplot\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, ElectraTokenizer, ElectraModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from Utils.dataset import *\n",
    "from Utils.utils import *\n",
    "from Models.BertClf import *\n",
    "from Models.LstmClf import *\n",
    "from Models.ElectraClf import *\n",
    "\n",
    "#################################################################################################################\n",
    "# Library Version\n",
    "#################################################################################################################\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")\n",
    "print(f\"matplotlib version: {mpl.__version__}\")\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "\n",
    "#################################################################################################################\n",
    "# Reproducible\n",
    "#################################################################################################################\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "\n",
    "#################################################################################################################\n",
    "# Hyperparameters Setting\n",
    "#################################################################################################################\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model\n",
    "parser.add_argument('--model', type=str, default='ELECTRA', help='BERT, BILSTM, ELECTRA')\n",
    "parser.add_argument('--sent_embedding', type=int, default=0, help='0: CLS, 1: 4-layer concat')\n",
    "parser.add_argument('--hidden_dim', type=int, default=768, help='BERT or ELECTRA: hidden dimension of classifier, BILSTM: hidden dimension of lstm')\n",
    "parser.add_argument('--num_layer', type=int, default=2, help='BILSTM: number of layers of lstm')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='dropout ratio')\n",
    "\n",
    "# training\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--gpu', type=int, default=0, help='0,1,2,3')\n",
    "parser.add_argument('--max_epoch', type=int, default=1)\n",
    "parser.add_argument('--save', type=int, default=1, help='0: false, 1:true')\n",
    "parser.add_argument('--lr_pretrained', type=float, default=1e-05, help='learning rate, 5e-5, 3e-5 or 2e-5')\n",
    "parser.add_argument('--lr_clf', type=float, default=0.0001, help='learning rate, 5e-5, 3e-5 or 2e-5')\n",
    "parser.add_argument('--freeze_pretrained', type=int, default=0, help='0: false, 1:true')\n",
    "parser.add_argument('--eps', type=float, default=1e-8, help='epsilon for AdamW, 1e-8')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay for AdamW, 5e-4')\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--data_path', type=str, default='./Dataset')\n",
    "parser.add_argument('--save_model_path', type=str, default='./Saved_models')\n",
    "parser.add_argument('--save_submission_path', type=str, default='./Submissions')\n",
    "parser.add_argument('--max_len', type=int, default=50, help='max length of the sentence')\n",
    "parser.add_argument('--aug', type=int, default=0, help='0: false, 1: true(ru)')\n",
    "parser.add_argument('--split_ratio', type=int, default=3, help='k/10, k in [1,2,3]')\n",
    "parser.add_argument('--author', type=str, default='jh')\n",
    "\n",
    "\n",
    "#     opt = parser.parse_args() # in .py env\n",
    "opt, _ = parser.parse_known_args() # in .ipynb env\n",
    "\n",
    "#################################################################################################################\n",
    "# Training Device\n",
    "#################################################################################################################\n",
    "device = torch.device(\"cuda:\" + str(opt.gpu)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print(f'training device: {device, torch.cuda.get_device_name()}')\n",
    "curr_time = time.localtime()\n",
    "signature = f\"{opt.author}_{opt.model}_{curr_time.tm_mon}M_{curr_time.tm_mday}D_{curr_time.tm_hour}H_{curr_time.tm_min}M\"\n",
    "opt.signature = signature\n",
    "print(f'signature: {signature}')\n",
    "with open('./Saved_models/' + signature + '_opt.txt', 'w') as f:\n",
    "    json.dump(opt.__dict__, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e72623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:05:13.685792Z",
     "start_time": "2021-05-31T07:05:13.668955Z"
    },
    "code_folding": [
     3,
     140,
     189
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# Train and Evaluate\n",
    "#################################################################################################################\n",
    "def train_fn(model,\n",
    "             optimizer,\n",
    "             scheduler,\n",
    "             loss_fn,\n",
    "             train_dataloader,\n",
    "             valid_dataloader=None,\n",
    "             evaluation=False):\n",
    "    \"\"\"\n",
    "    Train the BertClassifier model with early stop trick.\n",
    "    \n",
    "    :param model: untrained model\n",
    "    :param train_dataloader: dataloader which is obtained by data_load method\n",
    "    :param valid_dataloader: dataloader which is obtained by data_load method\n",
    "    :param epochs: opt.max_epoch [int]\n",
    "    :param evaluation: [bool]\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    es_eval_dict = {\n",
    "        \"epoch\": 0,\n",
    "        \"train_loss\": 0,\n",
    "        \"valid_loss\": 0,\n",
    "        \"valid_acc\": 0\n",
    "    }  # early stop\n",
    "    for epoch_i in range(opt.max_epoch):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(\n",
    "            f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "        )\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            # Load batch to GPU\n",
    "            b_ids_tsr, b_masks_tsr, b_labels_tsr = tuple(\n",
    "                tsrs.to(device) for tsrs in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            if opt.model == \"BILSTM\":\n",
    "                logits = model(b_ids_tsr)\n",
    "            else:\n",
    "                logits = model(b_ids_tsr, b_masks_tsr)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels_tsr)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0\n",
    "                    and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(\n",
    "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\"\n",
    "                )\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\" * 70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        model_save_path = str(\n",
    "            opt.save_model_path) + \"/\" + opt.signature + '.model'\n",
    "        if evaluation == True:\n",
    "            previous_valid_acc = es_eval_dict[\"valid_acc\"]  # early stop\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            valid_loss, valid_acc = evaluate_fn(model, loss_fn,\n",
    "                                                valid_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(\n",
    "                f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {valid_loss:^10.6f} | {valid_acc:^9.2f} | {time_elapsed:^9.2f}\"\n",
    "            )\n",
    "            print(\"-\" * 70)\n",
    "            if previous_valid_acc < valid_acc:\n",
    "                es_eval_dict[\"epoch\"] = epoch_i\n",
    "                es_eval_dict[\"train_loss\"] = avg_train_loss\n",
    "                es_eval_dict[\"valid_loss\"] = valid_loss\n",
    "                es_eval_dict[\"valid_acc\"] = valid_acc\n",
    "                if opt.save == 1:\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    print('\\tthe model is improved... save at',\n",
    "                          model_save_path)\n",
    "        print(\"\\n\")\n",
    "    print(\"Final results table\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\n",
    "        f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "    )\n",
    "    final_epoch, final_train_loss, final_valid_loss, final_valid_acc = es_eval_dict[\n",
    "        \"epoch\"], es_eval_dict[\"train_loss\"], es_eval_dict[\n",
    "            \"valid_loss\"], es_eval_dict[\"valid_acc\"]\n",
    "    print(\n",
    "        f\"{final_epoch + 1:^7} | {'-':^7} | {final_train_loss:^12.6f} | {final_valid_loss:^10.6f} | {final_valid_acc:^9.2f} | {0:^9.2f}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Training complete!\")\n",
    "    return model, final_train_loss, final_valid_loss, final_valid_acc\n",
    "\n",
    "\n",
    "def evaluate_fn(model, loss_fn, valid_dataloader):\n",
    "    \"\"\"\n",
    "    After the completion of each training epoch, measure the model's performance on our validation set.\n",
    "    \n",
    "    :param model: trained model\n",
    "    :param valid_dataloader: dataloader which is obtained by data_load method\n",
    "    \n",
    "    :return valid_loss: validation loss [array]\n",
    "    :return valid_acc: validation accuracy [array]\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in valid_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_ids_tsr, b_masks_tsr, b_labels_tsr = tuple(\n",
    "            t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            if opt.model == \"BILSTM\":\n",
    "                logits = model(b_ids_tsr)\n",
    "            else:\n",
    "                logits = model(b_ids_tsr, b_masks_tsr)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels_tsr)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels_tsr).cpu().numpy().mean() * 100\n",
    "        valid_acc.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    valid_acc = np.mean(valid_acc)\n",
    "\n",
    "    return valid_loss, valid_acc\n",
    "\n",
    "\n",
    "def cross_validation(full_dataset=None, n_splits=5):\n",
    "    \"\"\"Define a cross validation function\n",
    "    \"\"\"\n",
    "    train_loss_list, valid_loss_list, valid_acc_list = [], [], []\n",
    "    full_ids = full_dataset.ids_tsr.detach().cpu().numpy()\n",
    "    full_labels = full_dataset.labels.detach().cpu().numpy()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=False)\n",
    "    for i, idx in enumerate(skf.split(full_ids, full_labels)):\n",
    "        print(f\"Start {i}-th cross validation...\\n\")\n",
    "        train_indices, valid_indices = idx[0], idx[1]\n",
    "        print(train_indices)\n",
    "        print(valid_indices)\n",
    "\n",
    "        train_subset = torch.utils.data.dataset.Subset(full_dataset,\n",
    "                                                       train_indices)\n",
    "        valid_subset = torch.utils.data.dataset.Subset(full_dataset,\n",
    "                                                       valid_indices)\n",
    "\n",
    "        print(\n",
    "            f\"len of train set: {len(train_subset)}, len of valid set: {len(valid_subset)}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_subset,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # Specify the loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize the model\n",
    "        untrained_model, optimizer, scheduler = initialize_model(\n",
    "            opt, len(train_dataloader), device)\n",
    "\n",
    "        _, train_loss, valid_loss, valid_acc = train_fn(untrained_model,\n",
    "                                                        optimizer,\n",
    "                                                        scheduler,\n",
    "                                                        loss_fn,\n",
    "                                                        train_dataloader,\n",
    "                                                        valid_dataloader,\n",
    "                                                        evaluation=True)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        print(f\"...Complete {i}-th cross validation\\n\")\n",
    "    train_loss_arr = np.array(train_loss_list)\n",
    "    valid_loss_arr = np.array(valid_loss_list)\n",
    "    valid_acc_arr = np.array(valid_acc_list)\n",
    "    valid_avg_score = np.mean(valid_acc_arr)\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average valid accuracy: {valid_avg_score}\")\n",
    "    print(\"=\" * 60)\n",
    "    return train_loss_arr, valid_loss_arr, valid_acc_arr, valid_avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698917f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:05:13.693763Z",
     "start_time": "2021-05-31T07:05:13.687255Z"
    }
   },
   "outputs": [],
   "source": [
    "# # k-cross validation\n",
    "# full_dataset = FullDataset(opt)\n",
    "# train_loss_arr, valid_loss_arr, valid_acc_arr, valid_avg_score = cross_validation(\n",
    "#     full_dataset=full_dataset,\n",
    "#     n_splits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2405e5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:08:38.794967Z",
     "start_time": "2021-05-31T07:05:13.694855Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_ids_tsr.shape: torch.Size([6291, 50])\n",
      "train_X_masks_tsr.shape: torch.Size([6291, 50])\n",
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_X_ids_tsr.shape: torch.Size([3262, 50])\n",
      "valid_X_masks_tsr.shape: torch.Size([3262, 50])\n",
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X_ids_tsr.shape: torch.Size([4311, 50])\n",
      "test_X_masks_tsr.shape: torch.Size([4311, 50])\n",
      "num of train_loader: 6291\n",
      "num of valid_loader: 3262\n",
      "num of test_loader: 4311\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.565662   |     -      |     -     |   6.68   \n",
      "   1    |   40    |   1.583780   |     -      |     -     |   8.51   \n",
      "   1    |   60    |   1.567426   |     -      |     -     |   8.93   \n",
      "   1    |   80    |   1.560138   |     -      |     -     |   6.40   \n",
      "   1    |   100   |   1.545085   |     -      |     -     |   8.89   \n",
      "   1    |   120   |   1.522972   |     -      |     -     |   8.87   \n",
      "   1    |   140   |   1.418879   |     -      |     -     |   7.23   \n",
      "   1    |   160   |   1.337390   |     -      |     -     |   6.46   \n",
      "   1    |   180   |   1.206698   |     -      |     -     |   8.62   \n",
      "   1    |   200   |   1.184074   |     -      |     -     |   9.03   \n",
      "   1    |   220   |   1.296813   |     -      |     -     |   6.78   \n",
      "   1    |   240   |   1.217612   |     -      |     -     |   9.04   \n",
      "   1    |   260   |   1.144776   |     -      |     -     |   8.94   \n",
      "   1    |   280   |   1.185040   |     -      |     -     |   8.02   \n",
      "   1    |   300   |   1.183917   |     -      |     -     |   7.27   \n",
      "   1    |   320   |   1.134665   |     -      |     -     |   9.02   \n",
      "   1    |   340   |   1.187815   |     -      |     -     |   9.05   \n",
      "   1    |   360   |   1.152749   |     -      |     -     |   6.36   \n",
      "   1    |   380   |   1.164071   |     -      |     -     |   8.20   \n",
      "   1    |   393   |   1.114222   |     -      |     -     |   5.51   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   1.317873   |  1.106455  |   50.12   |  180.10  \n",
      "----------------------------------------------------------------------\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_31D_16H_5M.model\n",
      "\n",
      "\n",
      "Final results table\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "   1    |    -    |   1.317873   |  1.106455  |   50.12   |   0.00   \n",
      "----------------------------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the DataLoaders\n",
    "train_dataloader, valid_dataloader, test_dataloader = data_load(opt)\n",
    "\n",
    "# Specify the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the model\n",
    "untrained_model, optimizer, scheduler = initialize_model(opt, len(train_dataloader), device)\n",
    "\n",
    "trained_model, _, _, _ = train_fn(untrained_model, optimizer, scheduler, loss_fn, train_dataloader, valid_dataloader=valid_dataloader, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0774c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:10:17.126847Z",
     "start_time": "2021-05-31T07:08:38.797259Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_X_ids_tsr.shape: torch.Size([8563, 50])\n",
      "full_X_masks_tsr.shape: torch.Size([8563, 50])\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.601295   |     -      |     -     |   5.76   \n",
      "   1    |   40    |   1.584488   |     -      |     -     |   9.05   \n",
      "   1    |   60    |   1.560860   |     -      |     -     |   9.01   \n",
      "   1    |   80    |   1.534742   |     -      |     -     |   7.46   \n",
      "   1    |   100   |   1.518987   |     -      |     -     |   6.99   \n",
      "   1    |   120   |   1.429041   |     -      |     -     |   4.43   \n",
      "   1    |   140   |   1.328610   |     -      |     -     |   2.08   \n",
      "   1    |   160   |   1.282812   |     -      |     -     |   2.12   \n",
      "   1    |   180   |   1.177814   |     -      |     -     |   2.08   \n",
      "   1    |   200   |   1.173598   |     -      |     -     |   2.33   \n",
      "   1    |   220   |   1.166474   |     -      |     -     |   2.13   \n",
      "   1    |   240   |   1.180503   |     -      |     -     |   2.36   \n",
      "   1    |   260   |   1.078665   |     -      |     -     |   2.07   \n",
      "   1    |   280   |   1.135250   |     -      |     -     |   2.04   \n",
      "   1    |   300   |   1.135031   |     -      |     -     |   2.04   \n",
      "   1    |   320   |   1.159164   |     -      |     -     |   2.03   \n",
      "   1    |   340   |   1.079746   |     -      |     -     |   2.07   \n",
      "   1    |   360   |   1.098773   |     -      |     -     |   2.02   \n",
      "   1    |   380   |   1.132587   |     -      |     -     |   2.07   \n",
      "   1    |   400   |   1.143240   |     -      |     -     |   2.05   \n",
      "   1    |   420   |   1.132164   |     -      |     -     |   2.09   \n",
      "   1    |   440   |   1.128913   |     -      |     -     |   2.13   \n",
      "   1    |   460   |   1.138943   |     -      |     -     |   2.02   \n",
      "   1    |   480   |   1.173828   |     -      |     -     |   2.04   \n",
      "   1    |   500   |   1.154433   |     -      |     -     |   2.04   \n",
      "   1    |   520   |   1.111603   |     -      |     -     |   2.07   \n",
      "   1    |   535   |   1.064348   |     -      |     -     |   1.52   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Final results table\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "   1    |    -    |   0.000000   |  0.000000  |   0.00    |   0.00   \n",
      "----------------------------------------------------------------------\n",
      "Training complete!\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_31D_16H_5M_full.model\n"
     ]
    }
   ],
   "source": [
    "full_dataloader = data_load(opt, flag=\"full\")\n",
    "untrained_model, optimizer, scheduler = initialize_model(opt, len(train_dataloader), device)\n",
    "full_trained_model, _, _, _ = train_fn(untrained_model, optimizer, scheduler, loss_fn, full_dataloader, evaluation=False)\n",
    "model_save_path = str(opt.save_model_path) + \"/\" + opt.signature +'_full.model'\n",
    "if opt.save == 1: \n",
    "    torch.save(full_trained_model.state_dict(), model_save_path)\n",
    "    print('\\tthe model is improved... save at', model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e971c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:10:17.130991Z",
     "start_time": "2021-05-31T07:10:17.128662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the submission file\n",
    "# make_submission(trained_model, opt, device, test_dataloader)\n",
    "# make_submission(trained_model, opt, device, test_dataloader, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40c6f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:10:17.141431Z",
     "start_time": "2021-05-31T07:10:17.132941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109486085"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of parameters\n",
    "numOfparams(trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
