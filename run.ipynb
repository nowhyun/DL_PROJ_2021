{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7a60a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:32:47.997899Z",
     "start_time": "2021-05-17T06:32:46.548080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.2.4\n",
      "numpy version: 1.20.2\n",
      "seaborn version: 0.11.1\n",
      "matplotlib version: 3.4.1\n",
      "sklearn version: 0.24.2\n",
      "transformers version: 4.5.1\n",
      "torch version: 1.8.1+cu102\n",
      "training device: (device(type='cuda', index=1), 'TITAN Xp')\n",
      "signature: jh_ELECTRA_5M_17D_15H_32M\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(\"ggplot\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, ElectraTokenizer, ElectraModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from Utils.dataset import *\n",
    "from Utils.utils import *\n",
    "from Models.BertClf import *\n",
    "from Models.LstmClf import *\n",
    "from Models.ElectraClf import *\n",
    "\n",
    "#################################################################################################################\n",
    "# Library Version\n",
    "#################################################################################################################\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")\n",
    "print(f\"matplotlib version: {mpl.__version__}\")\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "\n",
    "#################################################################################################################\n",
    "# Reproducible\n",
    "#################################################################################################################\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "\n",
    "#################################################################################################################\n",
    "# Hyperparameters Setting\n",
    "#################################################################################################################\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model\n",
    "parser.add_argument('--model', type=str, default='ELECTRA', help='BERT, BILSTM, ELECTRA')\n",
    "parser.add_argument('--sent_embedding', type=int, default=0, help='0: CLS, 1: 4-layer concat')\n",
    "parser.add_argument('--hidden_dim', type=int, default=128, help='for wide models')\n",
    "parser.add_argument('--num_layer', type=int, default=2, help='for deep models')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout ratio')\n",
    "\n",
    "# training\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--gpu', type=int, default=1, help='0,1,2,3')\n",
    "parser.add_argument('--max_epoch', type=int, default=8)\n",
    "parser.add_argument('--save', type=int, default=1, help='0: false, 1:true')\n",
    "parser.add_argument('--optimizer', type=int, default=1, help='1: SGD, 2: RMSProp, 3: Adam')\n",
    "parser.add_argument('--lr_pretrained', type=float, default=0.00001, help='learning rate, 5e-5, 3e-5 or 2e-5')\n",
    "parser.add_argument('--lr_clf', type=float, default=0.001, help='learning rate, 5e-5, 3e-5 or 2e-5')\n",
    "parser.add_argument('--freeze_pretrained', type=int, default=0, help='0: false, 1:true')\n",
    "parser.add_argument('--eps', type=float, default=1e-8, help='epsilon, 1e-8')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='epsilon')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='epsilon')\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--data_path', type=str, default='./Dataset')\n",
    "parser.add_argument('--save_model_path', type=str, default='./Saved_models')\n",
    "parser.add_argument('--save_submission_path', type=str, default='./Submissions')\n",
    "parser.add_argument('--stratified', type=int, default=0, help='Incomplete; 0: false, 1: true')\n",
    "parser.add_argument('--n_classes', type=int, default=5, help='for balanced')\n",
    "parser.add_argument('--n_samples', type=int, default=5, help='for balanced')\n",
    "parser.add_argument('--max_len', type=int, default=50, help='max length of the sentence')\n",
    "parser.add_argument('--regexp', type=int, default=0, help='reg exp option 0 or 1')\n",
    "parser.add_argument('--aug', type=int, default=0, help='0: false, 1: true(ru)')\n",
    "parser.add_argument('--split_ratio', type=int, default=3, help='k/10')\n",
    "parser.add_argument('--author', type=str, default='jh')\n",
    "\n",
    "\n",
    "#     opt = parser.parse_args() # in .py env\n",
    "opt, _ = parser.parse_known_args() # in .ipynb env\n",
    "\n",
    "#################################################################################################################\n",
    "# Training Device\n",
    "#################################################################################################################\n",
    "device = torch.device(\"cuda:\" + str(opt.gpu)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print(f'training device: {device, torch.cuda.get_device_name()}')\n",
    "curr_time = time.localtime()\n",
    "signature = f\"{opt.author}_{opt.model}_{curr_time.tm_mon}M_{curr_time.tm_mday}D_{curr_time.tm_hour}H_{curr_time.tm_min}M\"\n",
    "opt.signature = signature\n",
    "print(f'signature: {signature}')\n",
    "with open('./Saved_models/' + signature + '_opt.txt', 'w') as f:\n",
    "    json.dump(opt.__dict__, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e72623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:32:48.034471Z",
     "start_time": "2021-05-17T06:32:48.002479Z"
    },
    "code_folding": [
     3,
     140,
     189
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# Train and Evaluate\n",
    "#################################################################################################################\n",
    "def train_fn(model,\n",
    "             optimizer,\n",
    "             scheduler,\n",
    "             loss_fn,\n",
    "             train_dataloader,\n",
    "             valid_dataloader=None,\n",
    "             evaluation=False):\n",
    "    \"\"\"\n",
    "    Train the BertClassifier model with early stop trick.\n",
    "    \n",
    "    :param model: untrained model\n",
    "    :param train_dataloader: dataloader which is obtained by data_load method\n",
    "    :param valid_dataloader: dataloader which is obtained by data_load method\n",
    "    :param epochs: opt.max_epoch [int]\n",
    "    :param evaluation: [bool]\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    es_eval_dict = {\n",
    "        \"epoch\": 0,\n",
    "        \"train_loss\": 0,\n",
    "        \"valid_loss\": 0,\n",
    "        \"valid_acc\": 0\n",
    "    }  # early stop\n",
    "    for epoch_i in range(opt.max_epoch):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(\n",
    "            f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "        )\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            # Load batch to GPU\n",
    "            b_ids_tsr, b_masks_tsr, b_labels_tsr = tuple(\n",
    "                tsrs.to(device) for tsrs in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            if opt.model == \"BILSTM\":\n",
    "                logits = model(b_ids_tsr)\n",
    "            else:\n",
    "                logits = model(b_ids_tsr, b_masks_tsr)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels_tsr)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0\n",
    "                    and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(\n",
    "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\"\n",
    "                )\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\" * 70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        model_save_path = str(\n",
    "            opt.save_model_path) + \"/\" + opt.signature + '.model'\n",
    "        if evaluation == True:\n",
    "            previous_valid_acc = es_eval_dict[\"valid_acc\"]  # early stop\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            valid_loss, valid_acc = evaluate_fn(model, loss_fn,\n",
    "                                                valid_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(\n",
    "                f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {valid_loss:^10.6f} | {valid_acc:^9.2f} | {time_elapsed:^9.2f}\"\n",
    "            )\n",
    "            print(\"-\" * 70)\n",
    "            if previous_valid_acc < valid_acc:\n",
    "                es_eval_dict[\"epoch\"] = epoch_i\n",
    "                es_eval_dict[\"train_loss\"] = avg_train_loss\n",
    "                es_eval_dict[\"valid_loss\"] = valid_loss\n",
    "                es_eval_dict[\"valid_acc\"] = valid_acc\n",
    "                if opt.save == 1:\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    print('\\tthe model is improved... save at',\n",
    "                          model_save_path)\n",
    "        print(\"\\n\")\n",
    "    print(\"Final results table\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\n",
    "        f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "    )\n",
    "    final_epoch, final_train_loss, final_valid_loss, final_valid_acc = es_eval_dict[\n",
    "        \"epoch\"], es_eval_dict[\"train_loss\"], es_eval_dict[\n",
    "            \"valid_loss\"], es_eval_dict[\"valid_acc\"]\n",
    "    print(\n",
    "        f\"{final_epoch + 1:^7} | {'-':^7} | {final_train_loss:^12.6f} | {final_valid_loss:^10.6f} | {final_valid_acc:^9.2f} | {0:^9.2f}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Training complete!\")\n",
    "    return model, final_train_loss, final_valid_loss, final_valid_acc\n",
    "\n",
    "\n",
    "def evaluate_fn(model, loss_fn, valid_dataloader):\n",
    "    \"\"\"\n",
    "    After the completion of each training epoch, measure the model's performance on our validation set.\n",
    "    \n",
    "    :param model: trained model\n",
    "    :param valid_dataloader: dataloader which is obtained by data_load method\n",
    "    \n",
    "    :return valid_loss: validation loss [array]\n",
    "    :return valid_acc: validation accuracy [array]\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in valid_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_ids_tsr, b_masks_tsr, b_labels_tsr = tuple(\n",
    "            t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            if opt.model == \"BILSTM\":\n",
    "                logits = model(b_ids_tsr)\n",
    "            else:\n",
    "                logits = model(b_ids_tsr, b_masks_tsr)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels_tsr)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels_tsr).cpu().numpy().mean() * 100\n",
    "        valid_acc.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    valid_acc = np.mean(valid_acc)\n",
    "\n",
    "    return valid_loss, valid_acc\n",
    "\n",
    "\n",
    "def cross_validation(full_dataset=None, n_splits=5):\n",
    "    \"\"\"Define a cross validation function\n",
    "    \"\"\"\n",
    "    train_loss_list, valid_loss_list, valid_acc_list = [], [], []\n",
    "    full_ids = full_dataset.ids_tsr.detach().cpu().numpy()\n",
    "    full_labels = full_dataset.labels.detach().cpu().numpy()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=False)\n",
    "    for i, idx in enumerate(skf.split(full_ids, full_labels)):\n",
    "        print(f\"Start {i}-th cross validation...\\n\")\n",
    "        train_indices, valid_indices = idx[0], idx[1]\n",
    "        print(train_indices)\n",
    "        print(valid_indices)\n",
    "\n",
    "        train_subset = torch.utils.data.dataset.Subset(full_dataset,\n",
    "                                                       train_indices)\n",
    "        valid_subset = torch.utils.data.dataset.Subset(full_dataset,\n",
    "                                                       valid_indices)\n",
    "\n",
    "        print(\n",
    "            f\"len of train set: {len(train_subset)}, len of valid set: {len(valid_subset)}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_subset,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # Specify the loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize the model\n",
    "        untrained_model, optimizer, scheduler = initialize_model(\n",
    "            opt, len(train_dataloader), device)\n",
    "\n",
    "        _, train_loss, valid_loss, valid_acc = train_fn(untrained_model,\n",
    "                                                        optimizer,\n",
    "                                                        scheduler,\n",
    "                                                        loss_fn,\n",
    "                                                        train_dataloader,\n",
    "                                                        valid_dataloader,\n",
    "                                                        evaluation=True)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        print(f\"...Complete {i}-th cross validation\\n\")\n",
    "    train_loss_arr = np.array(train_loss_list)\n",
    "    valid_loss_arr = np.array(valid_loss_list)\n",
    "    valid_acc_arr = np.array(valid_acc_list)\n",
    "    valid_avg_score = np.mean(valid_acc_arr)\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average valid accuracy: {valid_avg_score}\")\n",
    "    print(\"=\" * 60)\n",
    "    return train_loss_arr, valid_loss_arr, valid_acc_arr, valid_avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698917f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:32:48.042440Z",
     "start_time": "2021-05-17T06:32:48.036217Z"
    }
   },
   "outputs": [],
   "source": [
    "# # k-cross validation\n",
    "# full_dataset = FullDataset(opt)\n",
    "# train_loss_arr, valid_loss_arr, valid_acc_arr, valid_avg_score = cross_validation(\n",
    "#     full_dataset=full_dataset,\n",
    "#     n_splits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2405e5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:38:52.002145Z",
     "start_time": "2021-05-17T06:32:48.043783Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_ids_tsr.shape: torch.Size([5994, 50])\n",
      "train_X_masks_tsr.shape: torch.Size([5994, 50])\n",
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_X_ids_tsr.shape: torch.Size([2569, 50])\n",
      "valid_X_masks_tsr.shape: torch.Size([2569, 50])\n",
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X_ids_tsr.shape: torch.Size([4311, 50])\n",
      "test_X_masks_tsr.shape: torch.Size([4311, 50])\n",
      "num of train_loader: 5994\n",
      "num of valid_loader: 2569\n",
      "num of test_loader: 4311\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.583848   |     -      |     -     |   2.10   \n",
      "   1    |   40    |   1.558758   |     -      |     -     |   1.91   \n",
      "   1    |   60    |   1.337914   |     -      |     -     |   1.90   \n",
      "   1    |   80    |   1.229909   |     -      |     -     |   1.91   \n",
      "   1    |   100   |   1.158082   |     -      |     -     |   1.91   \n",
      "   1    |   120   |   1.135169   |     -      |     -     |   1.93   \n",
      "   1    |   140   |   1.193371   |     -      |     -     |   2.04   \n",
      "   1    |   160   |   1.151228   |     -      |     -     |   1.90   \n",
      "   1    |   180   |   1.094279   |     -      |     -     |   1.90   \n",
      "   1    |   200   |   1.097987   |     -      |     -     |   1.91   \n",
      "   1    |   220   |   1.109916   |     -      |     -     |   1.91   \n",
      "   1    |   240   |   1.051075   |     -      |     -     |   1.91   \n",
      "   1    |   260   |   1.120368   |     -      |     -     |   1.92   \n",
      "   1    |   280   |   1.139872   |     -      |     -     |   1.95   \n",
      "   1    |   300   |   1.074810   |     -      |     -     |   1.93   \n",
      "   1    |   320   |   1.050204   |     -      |     -     |   1.92   \n",
      "   1    |   340   |   1.061743   |     -      |     -     |   1.95   \n",
      "   1    |   360   |   1.036313   |     -      |     -     |   1.95   \n",
      "   1    |   374   |   1.127713   |     -      |     -     |   1.35   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   1.176183   |  1.059875  |   52.79   |   39.97  \n",
      "----------------------------------------------------------------------\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_17D_15H_32M.model\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   1.064807   |     -      |     -     |   2.09   \n",
      "   2    |   40    |   1.057932   |     -      |     -     |   1.93   \n",
      "   2    |   60    |   0.883088   |     -      |     -     |   2.00   \n",
      "   2    |   80    |   0.948749   |     -      |     -     |   1.99   \n",
      "   2    |   100   |   0.958513   |     -      |     -     |   1.94   \n",
      "   2    |   120   |   0.935151   |     -      |     -     |   1.96   \n",
      "   2    |   140   |   0.992820   |     -      |     -     |   2.00   \n",
      "   2    |   160   |   0.919930   |     -      |     -     |   1.96   \n",
      "   2    |   180   |   0.909073   |     -      |     -     |   1.97   \n",
      "   2    |   200   |   0.898594   |     -      |     -     |   1.99   \n",
      "   2    |   220   |   0.958785   |     -      |     -     |   2.01   \n",
      "   2    |   240   |   0.859478   |     -      |     -     |   2.03   \n",
      "   2    |   260   |   0.904311   |     -      |     -     |   2.07   \n",
      "   2    |   280   |   0.987026   |     -      |     -     |   2.06   \n",
      "   2    |   300   |   0.917885   |     -      |     -     |   1.96   \n",
      "   2    |   320   |   0.853886   |     -      |     -     |   1.99   \n",
      "   2    |   340   |   0.872212   |     -      |     -     |   2.05   \n",
      "   2    |   360   |   0.869676   |     -      |     -     |   2.07   \n",
      "   2    |   374   |   0.947947   |     -      |     -     |   1.39   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.933798   |  1.117744  |   53.19   |   41.82  \n",
      "----------------------------------------------------------------------\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_17D_15H_32M.model\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.989030   |     -      |     -     |   2.24   \n",
      "   3    |   40    |   0.877798   |     -      |     -     |   2.04   \n",
      "   3    |   60    |   0.851133   |     -      |     -     |   2.08   \n",
      "   3    |   80    |   0.813274   |     -      |     -     |   2.01   \n",
      "   3    |   100   |   0.832673   |     -      |     -     |   2.08   \n",
      "   3    |   120   |   0.775755   |     -      |     -     |   2.04   \n",
      "   3    |   140   |   0.853384   |     -      |     -     |   2.07   \n",
      "   3    |   160   |   0.740823   |     -      |     -     |   2.00   \n",
      "   3    |   180   |   0.769385   |     -      |     -     |   2.07   \n",
      "   3    |   200   |   0.765635   |     -      |     -     |   2.06   \n",
      "   3    |   220   |   0.874998   |     -      |     -     |   2.06   \n",
      "   3    |   240   |   0.747557   |     -      |     -     |   2.03   \n",
      "   3    |   260   |   0.785325   |     -      |     -     |   2.06   \n",
      "   3    |   280   |   0.842136   |     -      |     -     |   2.05   \n",
      "   3    |   300   |   0.833865   |     -      |     -     |   2.05   \n",
      "   3    |   320   |   0.722552   |     -      |     -     |   2.03   \n",
      "   3    |   340   |   0.751419   |     -      |     -     |   1.97   \n",
      "   3    |   360   |   0.761273   |     -      |     -     |   2.04   \n",
      "   3    |   374   |   0.811983   |     -      |     -     |   1.39   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.810979   |  1.103471  |   55.83   |   42.61  \n",
      "----------------------------------------------------------------------\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_17D_15H_32M.model\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.837928   |     -      |     -     |   2.19   \n",
      "   4    |   40    |   0.757947   |     -      |     -     |   2.06   \n",
      "   4    |   60    |   0.738845   |     -      |     -     |   2.08   \n",
      "   4    |   80    |   0.728748   |     -      |     -     |   2.02   \n",
      "   4    |   100   |   0.679176   |     -      |     -     |   2.03   \n",
      "   4    |   120   |   0.673772   |     -      |     -     |   2.08   \n",
      "   4    |   140   |   0.811058   |     -      |     -     |   2.05   \n",
      "   4    |   160   |   0.612703   |     -      |     -     |   2.06   \n",
      "   4    |   180   |   0.702954   |     -      |     -     |   2.02   \n",
      "   4    |   200   |   0.662855   |     -      |     -     |   1.99   \n",
      "   4    |   220   |   0.656768   |     -      |     -     |   2.02   \n",
      "   4    |   240   |   0.671140   |     -      |     -     |   2.06   \n",
      "   4    |   260   |   0.671389   |     -      |     -     |   2.05   \n",
      "   4    |   280   |   0.744816   |     -      |     -     |   2.03   \n",
      "   4    |   300   |   0.689657   |     -      |     -     |   2.06   \n",
      "   4    |   320   |   0.641075   |     -      |     -     |   2.06   \n",
      "   4    |   340   |   0.626327   |     -      |     -     |   2.03   \n",
      "   4    |   360   |   0.685301   |     -      |     -     |   2.08   \n",
      "   4    |   374   |   0.646252   |     -      |     -     |   1.35   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.697959   |  1.213731  |   54.93   |   42.67  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   0.721052   |     -      |     -     |   2.13   \n",
      "   5    |   40    |   0.663294   |     -      |     -     |   2.05   \n",
      "   5    |   60    |   0.577394   |     -      |     -     |   2.05   \n",
      "   5    |   80    |   0.589237   |     -      |     -     |   2.06   \n",
      "   5    |   100   |   0.638730   |     -      |     -     |   2.04   \n",
      "   5    |   120   |   0.611654   |     -      |     -     |   1.96   \n",
      "   5    |   140   |   0.665085   |     -      |     -     |   2.04   \n",
      "   5    |   160   |   0.508345   |     -      |     -     |   2.00   \n",
      "   5    |   180   |   0.587624   |     -      |     -     |   2.00   \n",
      "   5    |   200   |   0.564011   |     -      |     -     |   2.10   \n",
      "   5    |   220   |   0.598137   |     -      |     -     |   1.99   \n",
      "   5    |   240   |   0.564678   |     -      |     -     |   2.03   \n",
      "   5    |   260   |   0.603480   |     -      |     -     |   2.06   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |   280   |   0.627630   |     -      |     -     |   2.09   \n",
      "   5    |   300   |   0.600649   |     -      |     -     |   2.03   \n",
      "   5    |   320   |   0.508475   |     -      |     -     |   2.04   \n",
      "   5    |   340   |   0.548141   |     -      |     -     |   2.05   \n",
      "   5    |   360   |   0.590709   |     -      |     -     |   2.04   \n",
      "   5    |   374   |   0.622163   |     -      |     -     |   1.43   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.599461   |  1.308778  |   55.85   |   42.52  \n",
      "----------------------------------------------------------------------\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_17D_15H_32M.model\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   20    |   0.668035   |     -      |     -     |   2.14   \n",
      "   6    |   40    |   0.605135   |     -      |     -     |   2.05   \n",
      "   6    |   60    |   0.527578   |     -      |     -     |   2.04   \n",
      "   6    |   80    |   0.553830   |     -      |     -     |   2.04   \n",
      "   6    |   100   |   0.564119   |     -      |     -     |   2.04   \n",
      "   6    |   120   |   0.568816   |     -      |     -     |   2.09   \n",
      "   6    |   140   |   0.576598   |     -      |     -     |   2.01   \n",
      "   6    |   160   |   0.455220   |     -      |     -     |   2.03   \n",
      "   6    |   180   |   0.487355   |     -      |     -     |   2.09   \n",
      "   6    |   200   |   0.525081   |     -      |     -     |   2.05   \n",
      "   6    |   220   |   0.530505   |     -      |     -     |   2.03   \n",
      "   6    |   240   |   0.482947   |     -      |     -     |   2.03   \n",
      "   6    |   260   |   0.546823   |     -      |     -     |   2.10   \n",
      "   6    |   280   |   0.510935   |     -      |     -     |   2.02   \n",
      "   6    |   300   |   0.552569   |     -      |     -     |   2.07   \n",
      "   6    |   320   |   0.431762   |     -      |     -     |   2.01   \n",
      "   6    |   340   |   0.465954   |     -      |     -     |   2.09   \n",
      "   6    |   360   |   0.521256   |     -      |     -     |   2.03   \n",
      "   6    |   374   |   0.513310   |     -      |     -     |   1.39   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.531586   |  1.401121  |   54.89   |   42.72  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   20    |   0.578603   |     -      |     -     |   2.15   \n",
      "   7    |   40    |   0.593345   |     -      |     -     |   2.00   \n",
      "   7    |   60    |   0.525558   |     -      |     -     |   2.09   \n",
      "   7    |   80    |   0.431009   |     -      |     -     |   2.01   \n",
      "   7    |   100   |   0.520502   |     -      |     -     |   2.07   \n",
      "   7    |   120   |   0.488610   |     -      |     -     |   2.06   \n",
      "   7    |   140   |   0.553022   |     -      |     -     |   2.00   \n",
      "   7    |   160   |   0.365168   |     -      |     -     |   2.08   \n",
      "   7    |   180   |   0.453699   |     -      |     -     |   2.04   \n",
      "   7    |   200   |   0.419448   |     -      |     -     |   2.04   \n",
      "   7    |   220   |   0.477514   |     -      |     -     |   2.03   \n",
      "   7    |   240   |   0.413294   |     -      |     -     |   2.09   \n",
      "   7    |   260   |   0.424780   |     -      |     -     |   2.02   \n",
      "   7    |   280   |   0.472420   |     -      |     -     |   2.03   \n",
      "   7    |   300   |   0.465482   |     -      |     -     |   2.08   \n",
      "   7    |   320   |   0.395332   |     -      |     -     |   2.03   \n",
      "   7    |   340   |   0.430407   |     -      |     -     |   2.09   \n",
      "   7    |   360   |   0.468778   |     -      |     -     |   2.00   \n",
      "   7    |   374   |   0.553588   |     -      |     -     |   1.45   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.474315   |  1.477622  |   54.99   |   42.51  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   20    |   0.566918   |     -      |     -     |   2.22   \n",
      "   8    |   40    |   0.472893   |     -      |     -     |   2.08   \n",
      "   8    |   60    |   0.391550   |     -      |     -     |   2.01   \n",
      "   8    |   80    |   0.419191   |     -      |     -     |   2.07   \n",
      "   8    |   100   |   0.440502   |     -      |     -     |   2.02   \n",
      "   8    |   120   |   0.386606   |     -      |     -     |   2.05   \n",
      "   8    |   140   |   0.449976   |     -      |     -     |   2.05   \n",
      "   8    |   160   |   0.303493   |     -      |     -     |   2.04   \n",
      "   8    |   180   |   0.394470   |     -      |     -     |   2.09   \n",
      "   8    |   200   |   0.382996   |     -      |     -     |   2.00   \n",
      "   8    |   220   |   0.482164   |     -      |     -     |   2.04   \n",
      "   8    |   240   |   0.419821   |     -      |     -     |   2.07   \n",
      "   8    |   260   |   0.400795   |     -      |     -     |   2.09   \n",
      "   8    |   280   |   0.422358   |     -      |     -     |   2.01   \n",
      "   8    |   300   |   0.397455   |     -      |     -     |   2.07   \n",
      "   8    |   320   |   0.364516   |     -      |     -     |   2.03   \n",
      "   8    |   340   |   0.395080   |     -      |     -     |   2.03   \n",
      "   8    |   360   |   0.502351   |     -      |     -     |   2.07   \n",
      "   8    |   374   |   0.480512   |     -      |     -     |   1.43   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.424418   |  1.558060  |   52.81   |   42.65  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Final results table\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "   5    |    -    |   0.599461   |  1.308778  |   55.85   |   0.00   \n",
      "----------------------------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the DataLoaders\n",
    "train_dataloader, valid_dataloader, test_dataloader = data_load(opt)\n",
    "\n",
    "# Specify the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the model\n",
    "untrained_model, optimizer, scheduler = initialize_model(opt, len(train_dataloader), device)\n",
    "\n",
    "trained_model, _, _, _ = train_fn(untrained_model, optimizer, scheduler, loss_fn, train_dataloader, valid_dataloader=valid_dataloader, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0774c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:46:21.984756Z",
     "start_time": "2021-05-17T06:38:52.004010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "Apply the ElectraTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_X_ids_tsr.shape: torch.Size([8563, 50])\n",
      "full_X_masks_tsr.shape: torch.Size([8563, 50])\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.609650   |     -      |     -     |   2.15   \n",
      "   1    |   40    |   1.559224   |     -      |     -     |   2.01   \n",
      "   1    |   60    |   1.420479   |     -      |     -     |   2.01   \n",
      "   1    |   80    |   1.306388   |     -      |     -     |   2.00   \n",
      "   1    |   100   |   1.253354   |     -      |     -     |   2.06   \n",
      "   1    |   120   |   1.197492   |     -      |     -     |   2.04   \n",
      "   1    |   140   |   1.141037   |     -      |     -     |   2.08   \n",
      "   1    |   160   |   1.169511   |     -      |     -     |   2.00   \n",
      "   1    |   180   |   1.090636   |     -      |     -     |   2.18   \n",
      "   1    |   200   |   1.126110   |     -      |     -     |   2.16   \n",
      "   1    |   220   |   1.139038   |     -      |     -     |   2.00   \n",
      "   1    |   240   |   1.090989   |     -      |     -     |   2.20   \n",
      "   1    |   260   |   1.017346   |     -      |     -     |   2.01   \n",
      "   1    |   280   |   1.113306   |     -      |     -     |   2.15   \n",
      "   1    |   300   |   1.100532   |     -      |     -     |   2.05   \n",
      "   1    |   320   |   1.097828   |     -      |     -     |   2.02   \n",
      "   1    |   340   |   1.031797   |     -      |     -     |   2.09   \n",
      "   1    |   360   |   1.041676   |     -      |     -     |   2.02   \n",
      "   1    |   380   |   1.160115   |     -      |     -     |   2.06   \n",
      "   1    |   400   |   1.129121   |     -      |     -     |   2.06   \n",
      "   1    |   420   |   1.103445   |     -      |     -     |   1.97   \n",
      "   1    |   440   |   1.033930   |     -      |     -     |   2.00   \n",
      "   1    |   460   |   1.113007   |     -      |     -     |   2.04   \n",
      "   1    |   480   |   1.090602   |     -      |     -     |   2.07   \n",
      "   1    |   500   |   1.051597   |     -      |     -     |   2.04   \n",
      "   1    |   520   |   1.006402   |     -      |     -     |   2.03   \n",
      "   1    |   535   |   1.027500   |     -      |     -     |   1.51   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.999402   |     -      |     -     |   2.17   \n",
      "   2    |   40    |   0.993400   |     -      |     -     |   2.06   \n",
      "   2    |   60    |   0.980209   |     -      |     -     |   1.99   \n",
      "   2    |   80    |   0.936353   |     -      |     -     |   1.99   \n",
      "   2    |   100   |   0.967913   |     -      |     -     |   1.96   \n",
      "   2    |   120   |   0.984219   |     -      |     -     |   2.03   \n",
      "   2    |   140   |   0.919869   |     -      |     -     |   2.02   \n",
      "   2    |   160   |   0.909387   |     -      |     -     |   2.08   \n",
      "   2    |   180   |   0.915020   |     -      |     -     |   2.09   \n",
      "   2    |   200   |   0.898428   |     -      |     -     |   2.01   \n",
      "   2    |   220   |   0.944041   |     -      |     -     |   2.05   \n",
      "   2    |   240   |   0.971863   |     -      |     -     |   2.04   \n",
      "   2    |   260   |   0.875948   |     -      |     -     |   2.07   \n",
      "   2    |   280   |   0.934319   |     -      |     -     |   2.02   \n",
      "   2    |   300   |   0.914242   |     -      |     -     |   2.08   \n",
      "   2    |   320   |   0.918281   |     -      |     -     |   2.02   \n",
      "   2    |   340   |   0.867550   |     -      |     -     |   2.07   \n",
      "   2    |   360   |   0.842972   |     -      |     -     |   1.96   \n",
      "   2    |   380   |   0.913367   |     -      |     -     |   2.02   \n",
      "   2    |   400   |   0.935298   |     -      |     -     |   2.02   \n",
      "   2    |   420   |   0.953544   |     -      |     -     |   2.07   \n",
      "   2    |   440   |   0.864185   |     -      |     -     |   2.05   \n",
      "   2    |   460   |   0.940808   |     -      |     -     |   2.06   \n",
      "   2    |   480   |   0.928499   |     -      |     -     |   2.04   \n",
      "   2    |   500   |   0.937103   |     -      |     -     |   2.05   \n",
      "   2    |   520   |   0.835955   |     -      |     -     |   1.98   \n",
      "   2    |   535   |   0.928840   |     -      |     -     |   1.46   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.841284   |     -      |     -     |   2.16   \n",
      "   3    |   40    |   0.830498   |     -      |     -     |   1.97   \n",
      "   3    |   60    |   0.818648   |     -      |     -     |   2.05   \n",
      "   3    |   80    |   0.829407   |     -      |     -     |   2.04   \n",
      "   3    |   100   |   0.828368   |     -      |     -     |   2.04   \n",
      "   3    |   120   |   0.820374   |     -      |     -     |   2.10   \n",
      "   3    |   140   |   0.808786   |     -      |     -     |   2.07   \n",
      "   3    |   160   |   0.812912   |     -      |     -     |   2.14   \n",
      "   3    |   180   |   0.784668   |     -      |     -     |   2.15   \n",
      "   3    |   200   |   0.818018   |     -      |     -     |   2.22   \n",
      "   3    |   220   |   0.755601   |     -      |     -     |   2.11   \n",
      "   3    |   240   |   0.889155   |     -      |     -     |   2.22   \n",
      "   3    |   260   |   0.751043   |     -      |     -     |   2.20   \n",
      "   3    |   280   |   0.817485   |     -      |     -     |   2.25   \n",
      "   3    |   300   |   0.759958   |     -      |     -     |   2.24   \n",
      "   3    |   320   |   0.773596   |     -      |     -     |   2.25   \n",
      "   3    |   340   |   0.717738   |     -      |     -     |   2.14   \n",
      "   3    |   360   |   0.794059   |     -      |     -     |   2.14   \n",
      "   3    |   380   |   0.781751   |     -      |     -     |   2.12   \n",
      "   3    |   400   |   0.818625   |     -      |     -     |   2.17   \n",
      "   3    |   420   |   0.809370   |     -      |     -     |   2.08   \n",
      "   3    |   440   |   0.788158   |     -      |     -     |   2.19   \n",
      "   3    |   460   |   0.881882   |     -      |     -     |   2.17   \n",
      "   3    |   480   |   0.805584   |     -      |     -     |   2.17   \n",
      "   3    |   500   |   0.788074   |     -      |     -     |   2.13   \n",
      "   3    |   520   |   0.750481   |     -      |     -     |   2.11   \n",
      "   3    |   535   |   0.783702   |     -      |     -     |   1.56   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.757098   |     -      |     -     |   2.16   \n",
      "   4    |   40    |   0.747721   |     -      |     -     |   2.09   \n",
      "   4    |   60    |   0.758360   |     -      |     -     |   2.17   \n",
      "   4    |   80    |   0.744019   |     -      |     -     |   2.17   \n",
      "   4    |   100   |   0.808019   |     -      |     -     |   2.23   \n",
      "   4    |   120   |   0.720127   |     -      |     -     |   2.14   \n",
      "   4    |   140   |   0.719598   |     -      |     -     |   2.18   \n",
      "   4    |   160   |   0.677439   |     -      |     -     |   2.12   \n",
      "   4    |   180   |   0.677884   |     -      |     -     |   2.14   \n",
      "   4    |   200   |   0.694587   |     -      |     -     |   2.15   \n",
      "   4    |   220   |   0.697111   |     -      |     -     |   2.14   \n",
      "   4    |   240   |   0.782873   |     -      |     -     |   2.04   \n",
      "   4    |   260   |   0.632324   |     -      |     -     |   2.03   \n",
      "   4    |   280   |   0.699585   |     -      |     -     |   2.06   \n",
      "   4    |   300   |   0.668618   |     -      |     -     |   2.05   \n",
      "   4    |   320   |   0.675025   |     -      |     -     |   2.05   \n",
      "   4    |   340   |   0.655540   |     -      |     -     |   2.03   \n",
      "   4    |   360   |   0.665884   |     -      |     -     |   2.02   \n",
      "   4    |   380   |   0.712046   |     -      |     -     |   2.10   \n",
      "   4    |   400   |   0.734630   |     -      |     -     |   2.02   \n",
      "   4    |   420   |   0.749357   |     -      |     -     |   2.03   \n",
      "   4    |   440   |   0.660896   |     -      |     -     |   2.08   \n",
      "   4    |   460   |   0.761048   |     -      |     -     |   2.04   \n",
      "   4    |   480   |   0.705048   |     -      |     -     |   2.04   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4    |   500   |   0.725453   |     -      |     -     |   2.05   \n",
      "   4    |   520   |   0.644482   |     -      |     -     |   2.03   \n",
      "   4    |   535   |   0.712364   |     -      |     -     |   1.57   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   0.655164   |     -      |     -     |   2.17   \n",
      "   5    |   40    |   0.617663   |     -      |     -     |   2.05   \n",
      "   5    |   60    |   0.679227   |     -      |     -     |   2.05   \n",
      "   5    |   80    |   0.716927   |     -      |     -     |   2.02   \n",
      "   5    |   100   |   0.683116   |     -      |     -     |   2.05   \n",
      "   5    |   120   |   0.694284   |     -      |     -     |   2.05   \n",
      "   5    |   140   |   0.625582   |     -      |     -     |   2.04   \n",
      "   5    |   160   |   0.633303   |     -      |     -     |   2.09   \n",
      "   5    |   180   |   0.604332   |     -      |     -     |   2.03   \n",
      "   5    |   200   |   0.629945   |     -      |     -     |   2.04   \n",
      "   5    |   220   |   0.651585   |     -      |     -     |   2.05   \n",
      "   5    |   240   |   0.675385   |     -      |     -     |   2.04   \n",
      "   5    |   260   |   0.565093   |     -      |     -     |   2.04   \n",
      "   5    |   280   |   0.661435   |     -      |     -     |   2.07   \n",
      "   5    |   300   |   0.577621   |     -      |     -     |   2.03   \n",
      "   5    |   320   |   0.568274   |     -      |     -     |   2.05   \n",
      "   5    |   340   |   0.597263   |     -      |     -     |   2.05   \n",
      "   5    |   360   |   0.581654   |     -      |     -     |   2.10   \n",
      "   5    |   380   |   0.606857   |     -      |     -     |   2.10   \n",
      "   5    |   400   |   0.676544   |     -      |     -     |   2.06   \n",
      "   5    |   420   |   0.696318   |     -      |     -     |   2.06   \n",
      "   5    |   440   |   0.587708   |     -      |     -     |   2.06   \n",
      "   5    |   460   |   0.686630   |     -      |     -     |   2.04   \n",
      "   5    |   480   |   0.659558   |     -      |     -     |   2.04   \n",
      "   5    |   500   |   0.645803   |     -      |     -     |   2.03   \n",
      "   5    |   520   |   0.641397   |     -      |     -     |   2.04   \n",
      "   5    |   535   |   0.669988   |     -      |     -     |   1.52   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   20    |   0.621876   |     -      |     -     |   2.19   \n",
      "   6    |   40    |   0.532140   |     -      |     -     |   2.03   \n",
      "   6    |   60    |   0.567076   |     -      |     -     |   2.04   \n",
      "   6    |   80    |   0.600489   |     -      |     -     |   2.05   \n",
      "   6    |   100   |   0.624412   |     -      |     -     |   2.04   \n",
      "   6    |   120   |   0.617183   |     -      |     -     |   1.94   \n",
      "   6    |   140   |   0.533521   |     -      |     -     |   2.03   \n",
      "   6    |   160   |   0.595382   |     -      |     -     |   2.04   \n",
      "   6    |   180   |   0.603958   |     -      |     -     |   1.95   \n",
      "   6    |   200   |   0.544619   |     -      |     -     |   2.06   \n",
      "   6    |   220   |   0.563609   |     -      |     -     |   2.04   \n",
      "   6    |   240   |   0.612763   |     -      |     -     |   2.04   \n",
      "   6    |   260   |   0.538291   |     -      |     -     |   1.99   \n",
      "   6    |   280   |   0.608821   |     -      |     -     |   1.96   \n",
      "   6    |   300   |   0.573644   |     -      |     -     |   1.93   \n",
      "   6    |   320   |   0.487085   |     -      |     -     |   2.00   \n",
      "   6    |   340   |   0.502233   |     -      |     -     |   1.95   \n",
      "   6    |   360   |   0.545560   |     -      |     -     |   2.05   \n",
      "   6    |   380   |   0.567617   |     -      |     -     |   1.95   \n",
      "   6    |   400   |   0.574614   |     -      |     -     |   1.97   \n",
      "   6    |   420   |   0.663307   |     -      |     -     |   2.06   \n",
      "   6    |   440   |   0.590844   |     -      |     -     |   1.96   \n",
      "   6    |   460   |   0.636705   |     -      |     -     |   2.04   \n",
      "   6    |   480   |   0.652502   |     -      |     -     |   1.96   \n",
      "   6    |   500   |   0.613100   |     -      |     -     |   2.02   \n",
      "   6    |   520   |   0.580061   |     -      |     -     |   2.03   \n",
      "   6    |   535   |   0.618189   |     -      |     -     |   1.49   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   20    |   0.581346   |     -      |     -     |   2.03   \n",
      "   7    |   40    |   0.502457   |     -      |     -     |   1.96   \n",
      "   7    |   60    |   0.531734   |     -      |     -     |   1.95   \n",
      "   7    |   80    |   0.553999   |     -      |     -     |   1.97   \n",
      "   7    |   100   |   0.562384   |     -      |     -     |   2.03   \n",
      "   7    |   120   |   0.577243   |     -      |     -     |   2.07   \n",
      "   7    |   140   |   0.531578   |     -      |     -     |   1.93   \n",
      "   7    |   160   |   0.529662   |     -      |     -     |   2.09   \n",
      "   7    |   180   |   0.572626   |     -      |     -     |   1.97   \n",
      "   7    |   200   |   0.560359   |     -      |     -     |   2.02   \n",
      "   7    |   220   |   0.546401   |     -      |     -     |   2.06   \n",
      "   7    |   240   |   0.616665   |     -      |     -     |   2.05   \n",
      "   7    |   260   |   0.493050   |     -      |     -     |   1.96   \n",
      "   7    |   280   |   0.640410   |     -      |     -     |   1.99   \n",
      "   7    |   300   |   0.540072   |     -      |     -     |   2.07   \n",
      "   7    |   320   |   0.498109   |     -      |     -     |   1.98   \n",
      "   7    |   340   |   0.514785   |     -      |     -     |   2.03   \n",
      "   7    |   360   |   0.504035   |     -      |     -     |   1.95   \n",
      "   7    |   380   |   0.600669   |     -      |     -     |   2.01   \n",
      "   7    |   400   |   0.609540   |     -      |     -     |   1.99   \n",
      "   7    |   420   |   0.642334   |     -      |     -     |   2.04   \n",
      "   7    |   440   |   0.610672   |     -      |     -     |   2.00   \n",
      "   7    |   460   |   0.606987   |     -      |     -     |   2.07   \n",
      "   7    |   480   |   0.677281   |     -      |     -     |   2.04   \n",
      "   7    |   500   |   0.593549   |     -      |     -     |   2.09   \n",
      "   7    |   520   |   0.559388   |     -      |     -     |   2.03   \n",
      "   7    |   535   |   0.628007   |     -      |     -     |   1.45   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   20    |   0.572023   |     -      |     -     |   2.13   \n",
      "   8    |   40    |   0.512589   |     -      |     -     |   2.00   \n",
      "   8    |   60    |   0.525396   |     -      |     -     |   1.98   \n",
      "   8    |   80    |   0.539223   |     -      |     -     |   2.03   \n",
      "   8    |   100   |   0.576878   |     -      |     -     |   2.07   \n",
      "   8    |   120   |   0.559441   |     -      |     -     |   2.02   \n",
      "   8    |   140   |   0.543861   |     -      |     -     |   1.99   \n",
      "   8    |   160   |   0.546739   |     -      |     -     |   2.02   \n",
      "   8    |   180   |   0.558116   |     -      |     -     |   1.99   \n",
      "   8    |   200   |   0.556646   |     -      |     -     |   2.03   \n",
      "   8    |   220   |   0.522095   |     -      |     -     |   2.02   \n",
      "   8    |   240   |   0.621794   |     -      |     -     |   2.07   \n",
      "   8    |   260   |   0.513842   |     -      |     -     |   2.01   \n",
      "   8    |   280   |   0.577184   |     -      |     -     |   1.98   \n",
      "   8    |   300   |   0.544283   |     -      |     -     |   2.03   \n",
      "   8    |   320   |   0.535245   |     -      |     -     |   2.07   \n",
      "   8    |   340   |   0.494865   |     -      |     -     |   2.03   \n",
      "   8    |   360   |   0.534158   |     -      |     -     |   2.05   \n",
      "   8    |   380   |   0.589276   |     -      |     -     |   1.98   \n",
      "   8    |   400   |   0.587427   |     -      |     -     |   2.06   \n",
      "   8    |   420   |   0.640452   |     -      |     -     |   1.93   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8    |   440   |   0.581680   |     -      |     -     |   1.95   \n",
      "   8    |   460   |   0.626102   |     -      |     -     |   1.98   \n",
      "   8    |   480   |   0.650339   |     -      |     -     |   2.00   \n",
      "   8    |   500   |   0.623551   |     -      |     -     |   2.04   \n",
      "   8    |   520   |   0.576507   |     -      |     -     |   1.97   \n",
      "   8    |   535   |   0.565756   |     -      |     -     |   1.41   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Final results table\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "   1    |    -    |   0.000000   |  0.000000  |   0.00    |   0.00   \n",
      "----------------------------------------------------------------------\n",
      "Training complete!\n",
      "\tthe model is improved... save at ./Saved_models/jh_ELECTRA_5M_17D_15H_32M_full.model\n"
     ]
    }
   ],
   "source": [
    "# # Train the model on the whole training data\n",
    "# if opt.save == 1:\n",
    "#     model_save_path = str(opt.save_model_path) + \"/\" + opt.signature +'.model'\n",
    "#     trained_model.load_state_dict(torch.load(model_save_path))\n",
    "#     trained_model.to(device)\n",
    "# full_trained_model, _, _, _ = train_fn(trained_model, optimizer, scheduler, loss_fn, valid_dataloader, evaluation=False)\n",
    "# model_save_path = str(opt.save_model_path) + \"/\" + opt.signature +'_full.model'\n",
    "# if opt.save == 1: \n",
    "#     torch.save(full_trained_model.state_dict(), model_save_path) # 이거 학습안되 ...?\n",
    "#     print('\\tthe model is improved... save at', model_save_path)\n",
    "    \n",
    "full_dataloader = data_load(opt, flag=\"full\")\n",
    "untrained_model, optimizer, scheduler = initialize_model(opt, len(train_dataloader), device)\n",
    "full_trained_model, _, _, _ = train_fn(untrained_model, optimizer, scheduler, loss_fn, full_dataloader, evaluation=False)\n",
    "model_save_path = str(opt.save_model_path) + \"/\" + opt.signature +'_full.model'\n",
    "if opt.save == 1: \n",
    "    torch.save(full_trained_model.state_dict(), model_save_path)\n",
    "    print('\\tthe model is improved... save at', model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e971c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:46:36.775457Z",
     "start_time": "2021-05-17T06:46:21.986464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  Category\n",
      "0        0         3\n",
      "1        1         3\n",
      "2        2         1\n",
      "3        3         1\n",
      "4        4         0\n",
      "...    ...       ...\n",
      "4306  4306         0\n",
      "4307  4307         3\n",
      "4308  4308         3\n",
      "4309  4309         4\n",
      "4310  4310         3\n",
      "\n",
      "[4311 rows x 2 columns]\n",
      "\t...Save complete at ./Submissions/jh_ELECTRA_5M_17D_15H_32M.csv\n",
      "        Id  Category\n",
      "0        0         3\n",
      "1        1         3\n",
      "2        2         1\n",
      "3        3         2\n",
      "4        4         0\n",
      "...    ...       ...\n",
      "4306  4306         1\n",
      "4307  4307         3\n",
      "4308  4308         3\n",
      "4309  4309         4\n",
      "4310  4310         2\n",
      "\n",
      "[4311 rows x 2 columns]\n",
      "\t...Save complete at ./Submissions/jh_ELECTRA_5M_17D_15H_32M_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the submission file\n",
    "make_submission(trained_model, opt, device, test_dataloader)\n",
    "make_submission(trained_model, opt, device, test_dataloader, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40c6f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T06:46:36.782698Z",
     "start_time": "2021-05-17T06:46:36.777327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108990725"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of parameters\n",
    "numOfparams(trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
